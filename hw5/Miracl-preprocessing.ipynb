{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8552a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from tqdm import tqdm\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "import string\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0192786",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('miracl/miracl-corpus', 'ru')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5be37d1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['docid', 'title', 'text'],\n",
       "        num_rows: 9543918\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f7f91ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['docid', 'title', 'text'],\n",
       "    num_rows: 8835109\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def filter_short_texts(example):\n",
    "    return len(example['text']) >= 50 and len(example['text']) <= 100000\n",
    "\n",
    "filtered_data = dataset['train'].filter(filter_short_texts, desc=\"Filtering texts\")\n",
    "filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3939f32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    return text.translate(translator).lower()\n",
    "\n",
    "def generate_ngrams(text, n=13):\n",
    "    tokens = text.split()\n",
    "    ngrams = set([\" \".join(tokens[i:i+n]) for i in range(len(tokens) - n + 1)])\n",
    "    return ngrams\n",
    "\n",
    "def create_minhash(ngrams):\n",
    "    m = MinHash(num_perm=128)\n",
    "    for gram in ngrams:\n",
    "        m.update(gram.encode('utf8'))\n",
    "    return m\n",
    "\n",
    "def deduplicate_documents(dataset, similarity_threshold=0.8, ngram_size=13):\n",
    "    lsh = MinHashLSH(threshold=similarity_threshold, num_perm=128)\n",
    "    unique_docs = []\n",
    "    added_docs = set()\n",
    "\n",
    "    for i, doc in enumerate(dataset['text']):\n",
    "        preprocessed_text = preprocess_text(doc)\n",
    "        ngrams = generate_ngrams(preprocessed_text, n=ngram_size)\n",
    "        minhash = create_minhash(ngrams)\n",
    "        \n",
    "        if len(lsh.query(minhash)) == 0:\n",
    "            lsh.insert(f\"doc_{i}\", minhash)\n",
    "            unique_docs.append(doc)\n",
    "            added_docs.add(f\"doc_{i}\")\n",
    "    \n",
    "    return unique_docs\n",
    "\n",
    "filtered_dataset = deduplicate_documents(\n",
    "    dataset=filtered_data,  \n",
    "    similarity_threshold=0.8,  \n",
    "    ngram_size=13           \n",
    ")\n",
    "\n",
    "filtered_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949f3145",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model.eval()\n",
    "\n",
    "def split_text_into_chunks(text, chunk_size=1024):\n",
    "    words = text.split()  \n",
    "    chunks = [\" \".join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]\n",
    "    return chunks\n",
    "\n",
    "def calculate_perplexity(text, max_length=1024):\n",
    "\n",
    "    chunks = split_text_into_chunks(text, chunk_size=max_length // 2)  \n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    for chunk in chunks:\n",
    "        tokens = tokenizer.encode(chunk, return_tensors=\"pt\")\n",
    "        if tokens.size(1) > max_length:  \n",
    "            tokens = tokens[:, :max_length]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(tokens, labels=tokens)\n",
    "            total_loss += outputs.loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "    avg_loss = total_loss / num_batches if num_batches > 0 else 1e9 \n",
    "    perplexity = torch.exp(torch.tensor(avg_loss)).item()\n",
    "    return perplexity\n",
    "\n",
    "def filter_by_perplexity(data, perplexity_threshold=100, max_length=1024):\n",
    "\n",
    "    progress_bar = tqdm(data, desc=\"Calculating perplexity\", unit=\"text\")\n",
    "\n",
    "    def is_below_threshold(example):\n",
    "        perplexity = calculate_perplexity(example['text'], max_length=max_length)\n",
    "        return perplexity < perplexity_threshold\n",
    "\n",
    "    filtered_data = data.filter(\n",
    "        lambda example, idx: is_below_threshold(example),\n",
    "        with_indices=True,\n",
    "        desc=\"Filtering texts by perplexity\"\n",
    "    )\n",
    "\n",
    "    progress_bar.close()\n",
    "    return filtered_data\n",
    "\n",
    "filtered_dataset = filter_by_perplexity(\n",
    "    data=filtered_dataset,  \n",
    "    perplexity_threshold=100,  \n",
    "    max_length=1024  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f01d3849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7580594"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1e420d62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['_id', 'text'],\n",
       "    num_rows: 7580594\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NewDataset = Dataset.from_dict({\n",
    "    \"_id\": list(range(len(filtered_dataset))),  \n",
    "    \"text\": filtered_dataset                    \n",
    "})\n",
    "NewDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "417c34a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['_id', 'text'],\n",
       "        num_rows: 6822534\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['_id', 'text'],\n",
       "        num_rows: 758060\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NewDataset = NewDataset.train_test_split(test_size=0.1)\n",
    "NewDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b99819",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating perplexity:   0%|                                                                                                                                                                                                | 0/7580594 [12:05:55<?, ?text/s]\n"
     ]
    }
   ],
   "source": [
    "def deduplicate_train_by_test(train_dataset, test_dataset, similarity_threshold=0.8, ngram_size=13):\n",
    "\n",
    "    lsh = MinHashLSH(threshold=similarity_threshold, num_perm=128)\n",
    "\n",
    "    for i, doc in enumerate(test_dataset['text']):\n",
    "        preprocessed_text = preprocess_text(doc)\n",
    "        ngrams = generate_ngrams(preprocessed_text, n=ngram_size)\n",
    "        minhash = create_minhash(ngrams)\n",
    "        lsh.insert(f\"test_doc_{i}\", minhash)\n",
    "\n",
    "    filtered_train = []\n",
    "    for i, doc in enumerate(train_dataset['text']):\n",
    "        preprocessed_text = preprocess_text(doc)\n",
    "        ngrams = generate_ngrams(preprocessed_text, n=ngram_size)\n",
    "        minhash = create_minhash(ngrams)\n",
    "\n",
    "        if len(lsh.query(minhash)) == 0:\n",
    "            filtered_train.append(train_dataset[i])  \n",
    "\n",
    "    return filtered_train\n",
    "\n",
    "train_filtered = deduplicate_train_by_test(\n",
    "    train_dataset=NewDataset['train'],  \n",
    "    test_dataset=NewDataset['test'],    \n",
    "    similarity_threshold=0.8,  \n",
    "    ngram_size=13\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "efc1050f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_part = {'_id': [item['_id'] for item in train_filtered], \n",
    "             'text': [item['text'] for item in train_filtered]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d1bb51bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_part = Dataset.from_dict(train_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "576b9945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['_id', 'text'],\n",
       "        num_rows: 6822534\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['_id', 'text'],\n",
       "        num_rows: 758060\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NewDataset['train'] = train_part\n",
    "NewDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "22b96c41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68878226e8144201b741fd89a605057a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "931bd5a5c5c04419b407fd5e00fe7369",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/683 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3742705b31f94ee584cd4321dd0bff5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/683 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52e7b37ae17d4ad19c5e84d7a7ddf142",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/683 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b560b0068ad04634b6d2415d749cb5d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/683 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "702d861771ac4ee3bf913e2ace988483",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/683 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9741991c26f46f6b03f15cd7c807815",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/683 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9c43bdd2ec44f2e8ae5aab619822ac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/683 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f7b0f7d188a4fbcbe2ea88deab314a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/683 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7836009b1fe497ca59d79c8c83909a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/683 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "991d4c8a4fee4a03a4b46f1451cec779",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/683 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cc49cc1449e4602b20de5cadf2dbcf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3098c5dc3d5b403b92ce465002fd270a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/380 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5ad001a927048ca9a8947b3a0986913",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/380 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/kngrg/ru-miracl-cleaned/commit/16122a52b444856a5b5383cb67c934106f74bf13', commit_message='Upload dataset', commit_description='', oid='16122a52b444856a5b5383cb67c934106f74bf13', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/kngrg/ru-miracl-cleaned', endpoint='https://huggingface.co', repo_type='dataset', repo_id='kngrg/ru-miracl-cleaned'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NewDataset.push_to_hub(\"kngrg/ru-miracl-cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6215e352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cc30797b72745ca98c7a77236d4f00d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/6823 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved train split to output_json/train.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c94a1a5ed734c2e94f3e8069c4e3f3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/759 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved test split to output_json/test.json\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"output_json\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for split in NewDataset:\n",
    "    json_path = os.path.join(output_dir, f\"{split}.json\")\n",
    "    NewDataset[split].to_json(json_path)\n",
    "    print(f\"Saved {split} split to {json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d9611a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM-course",
   "language": "python",
   "name": "llm-course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
