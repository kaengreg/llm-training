{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zGiKaSVrkr90",
    "outputId": "78c5d02a-2b85-43ce-e9d4-7cbe32b96530"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Nov 10 20:25:07 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.4     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          Off | 00000000:CA:00.0 Off |                    0 |\n",
      "| N/A   26C    P0              92W / 400W |  28598MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BJ3hpGEKHEpG",
    "outputId": "f3d6a630-530f-4208-fa4c-ec7411730dd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.1.0)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.4)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.4.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.23.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2024.2.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.6.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.5.40)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: transformers==4.45.2 in /usr/local/lib/python3.10/dist-packages (4.45.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.2) (3.14.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.2) (0.23.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.2) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.2) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.2) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.2) (2023.12.25)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.2) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.2) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.2) (0.20.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.2) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.2) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.2) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.2) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.2) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.2) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.2) (2024.2.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.43.1)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.4.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.23.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2.6.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->bitsandbytes) (12.5.40)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->bitsandbytes) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.18.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: rouge-score in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score) (2.1.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score) (3.8.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.23.5)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.16.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (4.66.5)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: pymorphy2 in /usr/local/lib/python3.10/dist-packages (0.9.1)\n",
      "Requirement already satisfied: dawg-python>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from pymorphy2) (0.7.2)\n",
      "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /usr/local/lib/python3.10/dist-packages (from pymorphy2) (2.4.417127.4579844)\n",
      "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.10/dist-packages (from pymorphy2) (0.6.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.13.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.4)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.4.0)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.45.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.66.5)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (1.1.0)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.23.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (3.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2024.2.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.6.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->peft) (12.5.40)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.20.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install accelerate\n",
    "!pip install transformers==4.45.2\n",
    "!pip install bitsandbytes\n",
    "!pip install datasets\n",
    "!pip install rouge-score\n",
    "!pip install pymorphy2\n",
    "!pip install peft\n",
    "#!pip install flash_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'llmtf_open' already exists and is not an empty directory.\n",
      "/workdir/diploma-llm/hw3/llmtf_open\n",
      "--2024-11-10 20:25:27--  https://raw.githubusercontent.com/dialogue-evaluation/RuOpinionNE-2024/master/train.jsonl\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1291979 (1.2M) [text/plain]\n",
      "Saving to: ‘train.jsonl.19’\n",
      "\n",
      "train.jsonl.19      100%[===================>]   1.23M  --.-KB/s    in 0.1s    \n",
      "\n",
      "2024-11-10 20:25:28 (10.6 MB/s) - ‘train.jsonl.19’ saved [1291979/1291979]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/RefalMachine/llmtf_open\n",
    "%cd llmtf_open\n",
    "!wget https://raw.githubusercontent.com/dialogue-evaluation/RuOpinionNE-2024/master/train.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "WU1F0zQHbNMk"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import List, Dict\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "class ChatDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        original_records: List[Dict],\n",
    "        tokenizer: AutoTokenizer,\n",
    "        max_tokens_count: int,\n",
    "        sample_rate: float = 1.0,\n",
    "        only_target_loss: bool = True,\n",
    "        add_global_bos: bool = True,\n",
    "        add_global_eos: bool = True,\n",
    "        labels_pad_token_id: int = -100,\n",
    "    ):\n",
    "        self.original_records = original_records\n",
    "        self.sample_rate = sample_rate\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_tokens_count = max_tokens_count\n",
    "        self.only_target_loss = only_target_loss\n",
    "        self.labels_pad_token_id = labels_pad_token_id\n",
    "        self.add_global_bos = add_global_bos\n",
    "        self.add_global_eos = add_global_eos\n",
    "        self.is_printed = False\n",
    "\n",
    "        self.records = []\n",
    "        for record in tqdm(original_records):\n",
    "            if random.random() > self.sample_rate:\n",
    "                continue\n",
    "            tensors = self.convert_record(record)\n",
    "            if tensors is None:\n",
    "                continue\n",
    "            self.records.append(tensors)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.records)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.records[index]\n",
    "\n",
    "    def fill_template(self, message: str, inputs: Dict[str, str]) -> str:\n",
    "        \"\"\"Заполняет шаблон значениями из inputs.\"\"\"\n",
    "        return message.format(**inputs)\n",
    "\n",
    "    def get_tokens(self, messages):\n",
    "        tokens = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_special_tokens=False,\n",
    "            tokenize=True,\n",
    "            add_generation_prompt=False,\n",
    "        )\n",
    "        if tokens[0] == self.tokenizer.bos_token_id:\n",
    "            tokens = tokens[1:]\n",
    "        return tokens\n",
    "\n",
    "    def convert_record(self, record):\n",
    "        input_ids, labels = [], []\n",
    "\n",
    "        filled_messages = [\n",
    "            {\"content\": self.fill_template(msg[\"content\"], record[\"inputs\"]), \"role\": msg[\"role\"]}\n",
    "            for msg in record[\"messages\"]\n",
    "        ]\n",
    "\n",
    "        for i, message in enumerate(filled_messages):\n",
    "            if message['role'] == 'bot':\n",
    "                message['role'] = 'assistant'\n",
    "                filled_messages[i]['role'] = 'assistant'\n",
    "\n",
    "            message_input_ids = self.get_tokens([message])\n",
    "            message_labels = message_input_ids\n",
    "\n",
    "            if len(input_ids) + len(message_input_ids) > self.max_tokens_count - 2:\n",
    "                break\n",
    "\n",
    "            labels_mask = [self.labels_pad_token_id for _ in range(len(message_input_ids))]\n",
    "            if message[\"role\"] not in (\"assistant\", \"bot\", \"gpt\") and self.only_target_loss:\n",
    "                message_labels = labels_mask\n",
    "\n",
    "            input_ids.extend(message_input_ids)\n",
    "            labels.extend(message_labels)\n",
    "\n",
    "        if not input_ids:\n",
    "            return None\n",
    "\n",
    "        original_input_ids = self.get_tokens(filled_messages)\n",
    "        if input_ids != original_input_ids[: len(input_ids)]:\n",
    "            print(\"Mismatch found:\")\n",
    "            print(\"Generated input_ids:\", input_ids)\n",
    "            print(\"Original input_ids:\", original_input_ids[: len(input_ids)])\n",
    "        \n",
    "        assert input_ids == original_input_ids[: len(input_ids)]\n",
    "\n",
    "        if self.add_global_bos and input_ids[0] != self.tokenizer.bos_token_id:\n",
    "            input_ids.insert(0, self.tokenizer.bos_token_id)\n",
    "            labels.insert(0, self.labels_pad_token_id)\n",
    "\n",
    "        if input_ids[-2] == self.tokenizer.eos_token_id:\n",
    "            input_ids = input_ids[:-1]\n",
    "            labels = labels[:-1]\n",
    "\n",
    "        if self.add_global_eos and input_ids[-1] != self.tokenizer.eos_token_id:\n",
    "            input_ids.append(self.tokenizer.eos_token_id)\n",
    "            labels.append(self.tokenizer.eos_token_id)\n",
    "\n",
    "        if not self.is_printed:\n",
    "            print(input_ids)\n",
    "            print(labels)\n",
    "            print(\n",
    "                \"Full prompt:\" +\n",
    "                self.tokenizer.decode(input_ids, skip_special_tokens=False)\n",
    "            )\n",
    "            assert '\\n' in self.tokenizer.decode(input_ids, skip_special_tokens=False)\n",
    "            self.is_printed = True\n",
    "\n",
    "        input_ids = torch.LongTensor(input_ids)\n",
    "        labels = torch.LongTensor(labels)\n",
    "        attention_mask = input_ids.new_ones(input_ids.size())\n",
    "        assert (\n",
    "            input_ids.size(0)\n",
    "            == labels.size(0)\n",
    "            == attention_mask.size(0)\n",
    "            <= self.max_tokens_count\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"labels\": labels,\n",
    "            \"attention_mask\": attention_mask,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "spC3RQKabQcB",
    "outputId": "de636200-30b0-497c-be50-72c7c56cfdbb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['messages', 'inputs', 'outputs'],\n",
       "        num_rows: 3689\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['messages', 'inputs', 'outputs'],\n",
       "        num_rows: 923\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('kngrg/ru-QAmeleon')\n",
    "dataset = dataset['test']\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "9tew9-G5ak4b"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorForTokenClassification,\n",
    "    AutoConfig,\n",
    ")\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    logging,\n",
    "    TrainerCallback,\n",
    "    TrainerState,\n",
    "    TrainerControl,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n",
    "from peft import get_peft_model, LoraConfig\n",
    "import re\n",
    "from peft import prepare_model_for_kbit_training\n",
    "import codecs\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import json\n",
    "import copy\n",
    "from collections import OrderedDict, defaultdict\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from datasets import load_dataset, Dataset\n",
    "from typing import Dict, List, Tuple\n",
    "from llmtf.metrics import mean, metric_max_over_ground_truths, f1_macro_score\n",
    "import transformers.data.metrics.squad_metrics as squad_metrics\n",
    "import re\n",
    "from llmtf.base import Task, SimpleFewShotHFTask, LLM\n",
    "from difflib import SequenceMatcher\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "class QATask(SimpleFewShotHFTask):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.method = 'generate'\n",
    "        self.dataset_name = 'QAmeleon'\n",
    "        self._max_new_tokens = 64\n",
    "\n",
    "    @classmethod\n",
    "    def name(cls):\n",
    "        return 'kngrg/ru-QAmeleon'\n",
    "\n",
    "    def dataset_args(self) -> Dict:\n",
    "        return {'path': 'kngrg/ru-QAmeleon'}\n",
    "\n",
    "    def aggregation(self) -> Dict:\n",
    "        return {\n",
    "          \"f1\": mean,\n",
    "          \"em\": mean\n",
    "        }\n",
    "\n",
    "    def evaluate(self, sample, y_pred) -> Dict:\n",
    "        y_true = sample['outputs']['segment']\n",
    "        f1 = metric_max_over_ground_truths(squad_metrics.compute_f1, y_pred, y_true)\n",
    "        em = metric_max_over_ground_truths(squad_metrics.compute_exact, y_pred, y_true)\n",
    "\n",
    "        return {\n",
    "          \"f1\": f1,\n",
    "          \"em\": em\n",
    "        }\n",
    "\n",
    "    def test_split_name(self) -> str:\n",
    "        return 'test'\n",
    "\n",
    "    def prompt_split_name(self) -> str:\n",
    "        return 'prompt'\n",
    "\n",
    "    def create_messages(self, sample, with_answer=None) -> List[Dict]:\n",
    "        messages = sample['messages']\n",
    "        inputs = sample['inputs']\n",
    "        for m in messages:\n",
    "            m['content'] = m['content'].format(**inputs)\n",
    "        return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = QATask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 177,
     "referenced_widgets": [
      "95ae65903e3f4ce1959576b04bf6c4ff",
      "bd97ac6e56af45e3bf4b995c345f533c",
      "52ff31b1c5a9442991671d95d1e23fd4",
      "4f201002103948c18e94f54fb8567848",
      "a701759536274f55bbf8320578b2b3e0",
      "8e3ede7f99e34b008668b088f2565dfe",
      "6c07b9b025d447788341b416fec53147",
      "9c7e848c539b427f96706079bd1e64a2",
      "cde8795db54c47519c1403f239a58a13",
      "f78f74f1c69d42f8a14882d8aeb160ee",
      "dde7bb1edcf7401d9749fdb1e52e864c",
      "9187851156794fa8b41c36796b947f71",
      "6d74fc562e70461aa011fbee5bd7665e",
      "3ea94754c8b34bb3aad14fb4c9f0e594",
      "033b47e9310a4a229fde29cb2064dc6c",
      "b1028e64e9774542a29ef139202d1288",
      "245ff375638e41c8ac1e47eebdeb2b01",
      "07505c425cd5419ab4a41c67ada392ae",
      "938cd3787f8548ec8b8a36d3774f9eec",
      "3e4faae35d4b45d1b30a32e6d885066e",
      "2a584e7dc2b34cf8801fc5a5a1d7740a",
      "171d46a6115f47cba78446ae6371957d",
      "238e6f2d486d4408b0a7adbe6f1889f6",
      "0b3b4d659ddc4e91893e981e6bbf8ed0",
      "8546ebfc80954e5cb04fb8ea362be249",
      "36bcf267ff9942649671cce81e742aba",
      "5684ac71a1c34ffc8ea47fc9710b2f1c",
      "49d0c021442e43a2bc44800d49a7b2d6",
      "a3ade70ced7047118ccc7539dc27ae14",
      "c4d1ed5fb4684e3abe7a4526b283332f",
      "d948f33771c14bc4b2cacfb52d3ed9b6",
      "5f957cdd9aeb465d87609f3fb1780bd4",
      "51533dec6af44373bcf5a1c8d86e04d8",
      "866573d83f43442f987a7bb85a050a6c",
      "70f208a6e2264253bccd851db498b99f",
      "29aa7376c1fe4c7189369dfabdb5ae1a",
      "6ae57b9d8a814d7c971e3b445957f769",
      "18fa7c870f884d8cb72f09854e994ccd",
      "df2e3ec3bc264250b2619e154ca72e22",
      "9be3d99d069d4413a76242b0a22f9ea4",
      "5edae9ebae214b119966fdd638653c10",
      "3d3345c8c7c443de904983c69249b595",
      "511b81915b0644bc8f440e71a39f5164",
      "8e1b8f22ea7d4f06bcf7cfe0cf21a122",
      "04618654b00d486fb0864c2392d1172c",
      "d080bdb1cb5c44e791db6865632c9159",
      "0e89d83db14b4c2fb0a42dc0863953fd",
      "8fbc6af2924549a6966534b49c87d17a",
      "8d885ce1464b4405b87cf31a0c030c65",
      "e441c3c1bd3f4b79afc01555419afcb2",
      "bb3158f06a1e4536b157f704047b77da",
      "45f60f10ef744353a2c0f6ebfd3a3689",
      "d9b1df824f744d98ac9fbd87e7fb14ef",
      "47b0ad5e8a404e9b8cc468b278e3872f",
      "9048bd8bb75b426da09aa0bb96246053"
     ]
    },
    "id": "sQJOjWMiXnf9",
    "outputId": "23a7582e-f9bf-4e2f-e7e4-287198573e6c"
   },
   "outputs": [],
   "source": [
    "model_name = 'RefalMachine/ruadapt_qwen2.5_3B_ext_u48_instruct_v4'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rTCT7ZxJdbgU",
    "outputId": "9a29549b-6186-4432-f5f4-03f30f0d9803"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|███████▏                                                                                                                                                                                                             | 125/3689 [00:00<00:05, 657.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[147076, 882, 198, 54745, 38438, 9542, 102073, 105116, 71995, 7740, 93747, 102833, 13373, 100594, 25, 101036, 110789, 101728, 23934, 110810, 110646, 106429, 13373, 140652, 102677, 1532, 949, 11712, 35095, 15298, 6735, 4708, 1232, 220, 17, 21, 102085, 220, 16, 24, 16, 19, 100261, 100971, 110789, 101728, 23934, 110810, 110646, 106429, 13373, 140652, 102677, 1532, 11, 126363, 7952, 9706, 44075, 13999, 37013, 39900, 102105, 12769, 129098, 142569, 13289, 107667, 5591, 100449, 13373, 123724, 103417, 100046, 26, 220, 22, 18, 23, 123116, 118833, 105998, 108567, 26, 139544, 100269, 102453, 116244, 627, 147077, 198, 147076, 78191, 198, 60627, 48074, 25, 147077]\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 147076, 78191, 198, 60627, 48074, 25, 147077]\n",
      "Full prompt:<|im_start|>user\n",
      "Прочитайте данный текст и ответьте на вопрос: Когда произошла одна из крупнейших морских катастроф на британском флоте ? '''Текст''': 26 ноября 1914 года здесь произошла одна из крупнейших морских катастроф на британском флоте, линкор додредноудного типа «Болверк» взорвался на городском рейде; 738 моряков ВМС Великобритании погибли; погибшим был установлен монумент.\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Ответ:<|im_end|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3689/3689 [00:04<00:00, 764.38it/s]\n",
      "  8%|█████████████████▉                                                                                                                                                                                                     | 77/923 [00:00<00:01, 759.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[147076, 882, 198, 20976, 112778, 101064, 13373, 105116, 71995, 93747, 102833, 13373, 100594, 25, 101036, 57855, 121589, 102542, 111427, 101997, 5927, 107230, 107092, 949, 11712, 35095, 15298, 6735, 4708, 1232, 57855, 121589, 102542, 111427, 101997, 5927, 107230, 107092, 5927, 220, 16, 24, 23, 23, 100281, 7740, 5524, 100505, 91146, 106189, 100798, 100882, 100778, 115574, 6856, 13373, 100366, 100414, 43896, 107263, 13, 23784, 122622, 107230, 107092, 57855, 121589, 106166, 19175, 123856, 13, 125639, 105508, 4655, 23934, 57855, 20346, 100532, 19175, 104223, 48355, 100356, 101051, 146478, 108150, 627, 147077, 198, 147076, 78191, 198, 60627, 48074, 25, 147077]\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 147076, 78191, 198, 60627, 48074, 25, 147077]\n",
      "Full prompt:<|im_start|>user\n",
      "Опираясь на данный текст ответьте на вопрос: Когда Аруба впервые приняла участие в Олимпийских играх ? '''Текст''': Аруба впервые приняла участие в Олимпийских играх в 1988 году и с тех пор регулярно направляет своих атлетов на летние Игры. В зимних Олимпийских играх Аруба участия не принимала. Спортсмены из Арубы не завоевали ни одной олимпийской медали.\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Ответ:<|im_end|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 923/923 [00:01<00:00, 750.04it/s]\n"
     ]
    }
   ],
   "source": [
    "only_target_loss = True\n",
    "max_tokens_count = 512\n",
    "datasets = []\n",
    "for records in (dataset['train'], dataset['test']):\n",
    "    datasets.append(\n",
    "        ChatDataset(\n",
    "            records,\n",
    "            tokenizer,\n",
    "            max_tokens_count=max_tokens_count,\n",
    "            sample_rate=1.0,\n",
    "            only_target_loss=only_target_loss,\n",
    "            add_global_eos=False,\n",
    "            add_global_bos=False\n",
    "        )\n",
    "    )\n",
    "train_dataset, val_dataset = datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 710,
     "referenced_widgets": [
      "5d35aa519ce8413c8923084acdcfa499",
      "a0b54e4a8750456385e80446d7f54601",
      "015077447f0a46c2a37d2f875ad5b53e",
      "bb1fd8ed1914441d931833fc62169067",
      "9cb81f6968324d5ab4f78a64cedfb9e5",
      "be95cfb33136480f93990d7566a27911",
      "25fc22c63f984905a397e4ac8b8662b1",
      "9cac04e3d3f24f97abf114d98d50394c",
      "3d6d74fe2b1f4b61a14250194519601d",
      "9c51a5cdd0b44db48be7d1de0c76c2f9",
      "10c10dc39b3d4adbba73346065d8b73b",
      "8f232fb0ede84de9a365b6b726ba8cbb",
      "1440876e8d6c43aaa29f407c68eb5cd1",
      "3464e70c2f854b6392ed5a1bfb02daca",
      "1f8c68438fa94f8c8496be48a2d55f64",
      "ccf91b319ee24a94811997bb9f024d02",
      "3cc217df57ab40979bf4f119929b38f1",
      "3abb8498062540879e3ef6c20e9774f7",
      "5bb378b3714b48f59656ad4e7621733a",
      "a362656a5e3f45cca119b561e6b4a350",
      "c100d160848e4512aac018a45eec6903",
      "635d5e788d864557984eea6eacfa0b0d",
      "207dd3c14d8e4d8fb97b7a3b42ad9227",
      "d841378207f44ccd820388f60685717d",
      "29243643d89f4e0ca319fec7b8118dfb",
      "f203243bd12b48dda479d9f7363d8807",
      "a1a6a50ee0a147a6958b01091793b372",
      "5b336dc9eee14fb7a7afe059962b73c8",
      "efee8074b0ea4ef6b031d4658589da88",
      "4e9bb8cf54754924b8023b6e3201d9a9",
      "35f71306b4d8476795c78f594b05587b",
      "ecd9d5e549c14b1a97f91bf4ea9093cb",
      "ea7e4a62f8d04b5286545bf249c87236",
      "e18a89cccb9447c1969ff5caba7758fe",
      "a3d3a4fca96641558b35ef6703652ad6",
      "5b4b8e10185b4547bf8c48534aa7d55e",
      "e3530f9303614232968f1a7778733404",
      "9c6838e7f54a421b9452294bad074457",
      "ba5b0493fcab446aac1205538f84a6a4",
      "5b6e97fd6a5e44e492de8cba0e02291a",
      "3dccaf2444c64bb39f07a99d37121387",
      "ccdba3991b3441428bcbe760bbce08b4",
      "4c330fd683ca410f86d66cb33218f615",
      "1badc901c3994c63a579c5afb165c3a8",
      "7fe4a42fa02c4c2bb8e97d4f92ea6457",
      "161cb58c79a744848c149fbd5f88e6cb",
      "6c58a865fbf6432d97e9fa38c9875c26",
      "ad60c25ad4654dde835fbcce4c1b2f93",
      "263138fb51fe465ebf441a8986afe551",
      "3b76284c89de454a88b286bc5f7ce543",
      "f6c05e9daed1457a81b9d44b51acf283",
      "9bc8453ba9524369badf279778231371",
      "32486bf6a27548489659cd26ea2e38b5",
      "efe203f606ea45ad8c8b543b9be3b33f",
      "b05f9df8b3a94f548c1fe7604890ac5b",
      "10338be0394440c1a448571d2e1af417",
      "94ff4351ed5344a4a9cfc76401a1eabd",
      "938eabc2e6ab41639bd7ee7e078dd15e",
      "737947e6a69847a7bfb8fdfb9d40c482",
      "528f9cd5eef2417eaf9e55fac60a623d",
      "31380661c10f439ab842d526ed7c9eb4",
      "90b8f256b1eb4c0086dd83a1d79c27c1",
      "dbadabaf90324f9e9784623c31140aef",
      "7cccd19e14ac46d4878a35d747dd0d98",
      "3767e6894d2b47cebf2c240e11f9aa78",
      "e7599b1f248b4e4398218428dd0661a0",
      "2245907688e245d58a0ec34191ceec95",
      "88903effe0fd4d12b69c4aeb193db6f5",
      "c03850fd8a9a438590f1be2643d08dbb",
      "b61c25883f9e4a73b017bbff3296e369",
      "4a39f82ff9cf4cbd992f7e22d173f7ea",
      "731dadaa89f442239975d39d44d6d937",
      "7ed59602330b47ca97ea3b1d194131ba",
      "a8e090da89194ca7986cc4e96184dd7d",
      "fd4fde04ae064c2d8c582822e4ea6b9a",
      "88f7c4a4c38c4b66b2f91340abb8fc05",
      "07a84434bf6c4f22bb76c2a513a8fb27"
     ]
    },
    "id": "WO-tdmbI5Vwg",
    "outputId": "83ac8fcf-4ee8-41fe-8a6c-66f7717c7bcc"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "277b0e052ba34895887995a8ac8cfe70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(147097, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-35): 36 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2SdpaAttention(\n",
       "          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=2048, out_features=256, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=2048, out_features=256, bias=True)\n",
       "          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): Qwen2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear4bit(in_features=2048, out_features=11008, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=2048, out_features=11008, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=11008, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=147097, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=f\"cuda:0\",\n",
    "    torch_dtype=torch.float16,\n",
    "    attn_implementation=\"sdpa\",\n",
    ")\n",
    "prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ytjHd1qxfaHq",
    "outputId": "bef5bfc5-daba-45fc-e318-f1c49221fe21"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[147076,    882,    198,  20807,  19619, 129192,  48489, 101765,    445,\n",
       "          11237,     13, 147077,    198, 147076,  78191,    198]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [{'role': 'user', 'content': 'Напиши что такое LLM.'}]\n",
    "tokenizer.apply_chat_template(messages, return_tensors='pt', add_special_tokens=True, add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OsRI8Tj4fOwA",
    "outputId": "a8f8cace-74d5-4f82-8774-5180df1eaf3e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerationConfig {\n",
       "  \"bos_token_id\": 147075,\n",
       "  \"do_sample\": true,\n",
       "  \"eos_token_id\": 147077,\n",
       "  \"max_new_tokens\": 64,\n",
       "  \"pad_token_id\": 147075,\n",
       "  \"temperature\": 0.2,\n",
       "  \"top_k\": 40,\n",
       "  \"top_p\": 0.9\n",
       "}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GenerationConfig\n",
    "\n",
    "def generate(messages, model, tokenizer, generation_config):\n",
    "    input_ids = tokenizer.apply_chat_template(messages, return_tensors='pt', add_special_tokens=True, add_generation_prompt=True)\n",
    "    input_ids = input_ids.to(model.device)\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            input_ids,\n",
    "            generation_config=generation_config\n",
    "        )\n",
    "    outputs = []\n",
    "    for sample_output_ids, sample_input_ids in zip(output_ids, input_ids):\n",
    "        sample_output_ids = sample_output_ids[len(sample_input_ids):]\n",
    "        sample_output = tokenizer.decode(sample_output_ids, skip_special_tokens=True)\n",
    "        outputs.append(sample_output)\n",
    "\n",
    "    if len(outputs) == 1:\n",
    "        outputs = outputs[0]\n",
    "    return outputs\n",
    "\n",
    "\n",
    "\n",
    "generation_config = GenerationConfig.from_dict(\n",
    "    {\n",
    "        'top_k': 40,\n",
    "        'top_p': 0.9,\n",
    "        'temperature': 0.2,\n",
    "        'repetition_penalty': 1.0,\n",
    "        'max_new_tokens': 64,\n",
    "        'do_sample': True,\n",
    "        'pad_token_id': tokenizer.pad_token_id,\n",
    "        'bos_token_id': tokenizer.bos_token_id,\n",
    "        'eos_token_id': tokenizer.eos_token_id\n",
    "    }\n",
    ")\n",
    "generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "r6QZaeDHfwmo",
    "outputId": "ca2c5fd8-feed-4974-c41f-81b2cc198555"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LLM (Law Master of Laws) — это степень высшего образования, присуждаемая юридическим факультетам университетов по окончании трех- или четырехгодичного обучения. ЛLM является специализированным юридическим образованием, которое позволяет студентам углубить свои знания в определенной области права, например, в международном праве, корпоративном праве, интеллекту'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(messages, model, tokenizer, generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Rid85doezUI",
    "outputId": "2693344f-988e-40e8-8023-8a4d5bf0bcbb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type=None, inference_mode=False, r=16, target_modules={'q_proj', 'lm_head', 'k_proj', 'o_proj', 'v_proj'}, lora_alpha=16, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_config = {\n",
    "    \"r\": 16,\n",
    "    \"lora_alpha\": 16,\n",
    "    \"lora_dropout\": 0.0,\n",
    "    \"bias\": \"none\",\n",
    "    \"target_modules\": [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"lm_head\"]\n",
    "}\n",
    "lora_config = LoraConfig(**lora_config)\n",
    "lora_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "NdRq7JpKgFrv"
   },
   "outputs": [],
   "source": [
    "lora_config.modules_to_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "jsK7nWcdeiDn"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workdir/miniconds3/envs/llm-course/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:500: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['lm_head'] are part of the adapter. This can lead to complications, for example when merging the adapter or converting your model to formats other than safetensors. See for example https://github.com/huggingface/peft/issues/2018.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = get_peft_model(model, lora_config)\n",
    "if model.config.tie_word_embeddings and lora_config.modules_to_save is not None and 'lm_head' in lora_config.modules_to_save:\n",
    "    print('Tie embeddings')\n",
    "    assert 'embed_tokens' not in lora_config.modules_to_save\n",
    "    model.base_model.model.model.embed_tokens.weight = model.base_model.model.lm_head.modules_to_save[\"default\"].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qs4VhdlYgWNh",
    "outputId": "c69e8525-e971-438b-da23-ee14c68a0cb3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workdir/miniconds3/envs/llm-course/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "training_args = {\n",
    "    \"evaluation_strategy\": \"steps\",\n",
    "    \"per_device_train_batch_size\": 1,\n",
    "    \"per_device_eval_batch_size\": 1,\n",
    "    \"gradient_accumulation_steps\": 8,\n",
    "    \"eval_steps\": 16,\n",
    "    \"save_steps\": 128,\n",
    "    \"logging_steps\": 1,\n",
    "    \"learning_rate\": 0.00005,\n",
    "    \"num_train_epochs\": 1,\n",
    "    \"lr_scheduler_type\": \"cosine\",\n",
    "    \"warmup_steps\": 16,\n",
    "    \"bf16\": False,\n",
    "    \"fp16\": True,\n",
    "    \"optim\": \"paged_adamw_8bit\",\n",
    "    \"save_total_limit\": 1,\n",
    "    \"seed\": 1337,\n",
    "    \"max_grad_norm\": 1.0,\n",
    "    \"weight_decay\": 0.05\n",
    "}\n",
    "training_args = TrainingArguments(output_dir='./instruct', **training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LLoHlUYMgH3p",
    "outputId": "9bbc6d24-425a-4761-889b-ccea3f7374de"
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=8)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "if len(trainer.label_names) == 0:\n",
    "    trainer.label_names.append('labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "id": "smSpQW4BgwW2",
    "outputId": "8b3069f4-290e-47f4-ae61-a3dc6839d0bb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/workdir/miniconds3/envs/llm-course/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='461' max='461' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [461/461 1:19:18, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>3.333300</td>\n",
       "      <td>3.110122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.385000</td>\n",
       "      <td>0.255117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.006200</td>\n",
       "      <td>0.005532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.001242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.000723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.000544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.000444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.000260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>208</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>272</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>304</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>336</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>352</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>368</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>384</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>416</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>432</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>448</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000138</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workdir/miniconds3/envs/llm-course/lib/python3.10/site-packages/peft/utils/save_and_load.py:227: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/workdir/miniconds3/envs/llm-course/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/workdir/miniconds3/envs/llm-course/lib/python3.10/site-packages/peft/utils/save_and_load.py:227: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/workdir/miniconds3/envs/llm-course/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/workdir/miniconds3/envs/llm-course/lib/python3.10/site-packages/peft/utils/save_and_load.py:227: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/workdir/miniconds3/envs/llm-course/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/workdir/miniconds3/envs/llm-course/lib/python3.10/site-packages/peft/utils/save_and_load.py:227: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=461, training_loss=0.18462595219644398, metrics={'train_runtime': 4762.4959, 'train_samples_per_second': 0.775, 'train_steps_per_second': 0.097, 'total_flos': 7832200427285760.0, 'train_loss': 0.18462595219644398, 'epoch': 0.9997289238275956})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "id": "S5dxw71jhCSm",
    "outputId": "e4489a55-bc51-4d68-8111-63b9410eb027"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workdir/miniconds3/envs/llm-course/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/workdir/miniconds3/envs/llm-course/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'LLM (Law Master of Laws) — это профессиональный диплом, который присуждается студентам после успешного окончания четырех- или пяти-летний программы обучения в области права.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(messages, model, tokenizer, generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Nn814PE5h70t"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workdir/miniconds3/envs/llm-course/lib/python3.10/site-packages/peft/utils/save_and_load.py:227: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('models/ruQwen-lora/tokenizer_config.json',\n",
       " 'models/ruQwen-lora/special_tokens_map.json',\n",
       " 'models/ruQwen-lora/vocab.json',\n",
       " 'models/ruQwen-lora/merges.txt',\n",
       " 'models/ruQwen-lora/added_tokens.json',\n",
       " 'models/ruQwen-lora/tokenizer.json')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "save_directory = \"models/ruQwen-lora\"\n",
    "\n",
    "model.save_pretrained(save_directory)\n",
    "tokenizer.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = QATask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c0eebd1a8524a3cbe5b71c4352f4a33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workdir/miniconds3/envs/llm-course/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:500: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['lm_head'] are part of the adapter. This can lead to complications, for example when merging the adapter or converting your model to formats other than safetensors. See for example https://github.com/huggingface/peft/issues/2018.\n",
      "  warnings.warn(\n",
      "/workdir/miniconds3/envs/llm-course/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:396: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['lm_head'] are part of the adapter. This can lead to complications. You can opt to merge the adapter after cloning the weights (to untie the embeddings). You can untie the embeddings by loading the model with `tie_word_embeddings=False`. For example:\n",
      "```python\n",
      "from transformers import AutoModelForCausalLM\n",
      "\n",
      "# Load original tied model\n",
      "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b-it\", tie_word_embeddings=False)\n",
      "\n",
      "# Set the randomly initialized lm_head to the previously tied embeddings\n",
      "model.lm_head.weight.data = model.model.embed_tokens.weight.data.clone()\n",
      "\n",
      "# Save the untied model\n",
      "untied_model_dir = \"dir/for/untied/model\"\n",
      "model.save_pretrained(untied_model_dir)\n",
      "model.config.save_pretrained(untied_model_dir)\n",
      "\n",
      "# Now use the original model but in untied format\n",
      "model = AutoModelForCausalLM.from_pretrained(untied_model_dir)\n",
      "```\n",
      "\n",
      "  warnings.warn(\n",
      "INFO: 2024-11-10 21:45:24,274: llmtf.base.hfmodel: Set eos_token_id in generation_config to [147077]\n",
      "WARNING: 2024-11-10 21:45:24,275: llmtf.base.hfmodel: Global prefix is equal to empty string!\n",
      "INFO: 2024-11-10 21:45:24,275: llmtf.base.hfmodel: Model id: models/ruQwen-lora\n",
      "INFO: 2024-11-10 21:45:24,276: llmtf.base.hfmodel: Leading space: False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GenerationConfig {\n",
       "  \"bos_token_id\": 147075,\n",
       "  \"eos_token_id\": [\n",
       "    147077\n",
       "  ],\n",
       "  \"max_length\": 32768,\n",
       "  \"max_new_tokens\": 200,\n",
       "  \"pad_token_id\": 147075,\n",
       "  \"stop_strings\": [\n",
       "    \"<|im_end|>\"\n",
       "  ],\n",
       "  \"temperature\": 0.0,\n",
       "  \"top_k\": 40,\n",
       "  \"top_p\": 0.9\n",
       "}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llmtf.model import HFModel\n",
    "\n",
    "model_name_or_path = 'models/ruQwen-lora'\n",
    "model = HFModel(conversation_template_path='conversation_configs/qwen2.json', device_map='cuda:0', attn_implementation=\"sdpa\")\n",
    "model.from_pretrained(model_name_or_path)\n",
    "\n",
    "model.generation_config.max_new_tokens = 200\n",
    "model.generation_config.repetition_penalty = 1.0\n",
    "model.generation_config.do_sample = False\n",
    "model.generation_config.temperature = 0.0\n",
    "model.generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: 2024-11-10 21:45:24,285: llmtf.base.hfmodel: Updated generation_config.eos_token_id: [147077]\n",
      "INFO: 2024-11-10 21:45:24,286: llmtf.base.hfmodel: Updated generation_config.stop_strings: ['<|im_end|>']\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 1654.80it/s]\n",
      "INFO: 2024-11-10 21:45:26,762: llmtf.base.kngrg/ru-QAmeleon: Loading Dataset: 2.48s\n",
      "  0%|                                                                                                                                                                                                                                  | 0/50 [00:00<?, ?it/s]/workdir/miniconds3/envs/llm-course/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/workdir/miniconds3/envs/llm-course/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/workdir/miniconds3/envs/llm-course/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:612: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:03<00:00, 13.75it/s]\n",
      "INFO: 2024-11-10 21:45:30,401: llmtf.base.kngrg/ru-QAmeleon: Processing Dataset: 3.64s\n",
      "INFO: 2024-11-10 21:45:30,402: llmtf.base.kngrg/ru-QAmeleon: Results for kngrg/ru-QAmeleon:\n",
      "INFO: 2024-11-10 21:45:30,403: llmtf.base.kngrg/ru-QAmeleon: {'f1': 0.425, 'em': 0.425}\n"
     ]
    }
   ],
   "source": [
    "from llmtf.evaluator import Evaluator\n",
    "evaluator = Evaluator()\n",
    "\n",
    "evaluator.evaluate_dataset(\n",
    "    task=task,\n",
    "    model=model,\n",
    "    output_dir='./qa_qwenFT',\n",
    "    max_len=4000,\n",
    "    few_shot_count=0,\n",
    "    generation_config=None, \n",
    "    batch_size=4,\n",
    "    max_sample_per_dataset=200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"task_name\": \"kngrg/ru-QAmeleon\",\n",
      "    \"results\": {\n",
      "        \"f1\": 0.425,\n",
      "        \"em\": 0.425\n",
      "    },\n",
      "    \"leaderboard_result\": 0.425\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!cat ./qa_qwenFT/kngrg_ru-QAmeleon_total.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d6bdc9c01174ab49db804fa84511cbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: 2024-11-10 21:45:34,771: llmtf.base.hfmodel: Set eos_token_id in generation_config to [147077]\n",
      "WARNING: 2024-11-10 21:45:34,772: llmtf.base.hfmodel: Global prefix is equal to empty string!\n",
      "INFO: 2024-11-10 21:45:34,772: llmtf.base.hfmodel: Model id: RefalMachine/ruadapt_qwen2.5_3B_ext_u48_instruct_v4\n",
      "INFO: 2024-11-10 21:45:34,773: llmtf.base.hfmodel: Leading space: False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GenerationConfig {\n",
       "  \"bos_token_id\": 147075,\n",
       "  \"eos_token_id\": [\n",
       "    147077\n",
       "  ],\n",
       "  \"max_length\": 32768,\n",
       "  \"max_new_tokens\": 200,\n",
       "  \"pad_token_id\": 147075,\n",
       "  \"stop_strings\": [\n",
       "    \"<|im_end|>\"\n",
       "  ],\n",
       "  \"temperature\": 0.0,\n",
       "  \"top_k\": 40,\n",
       "  \"top_p\": 0.9,\n",
       "  \"trust_remote_code\": false\n",
       "}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llmtf.model import HFModel\n",
    "\n",
    "model_name_or_path = 'RefalMachine/ruadapt_qwen2.5_3B_ext_u48_instruct_v4'\n",
    "orig_model = HFModel(conversation_template_path='conversation_configs/qwen2.json', device_map='cuda:0', attn_implementation=\"sdpa\")\n",
    "orig_model.from_pretrained(model_name_or_path)\n",
    "\n",
    "orig_model.generation_config.max_new_tokens = 200\n",
    "orig_model.generation_config.repetition_penalty = 1.0\n",
    "orig_model.generation_config.do_sample = False\n",
    "orig_model.generation_config.temperature = 0.0\n",
    "orig_model.generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: 2024-11-10 21:45:34,780: llmtf.base.hfmodel: Updated generation_config.eos_token_id: [147077]\n",
      "INFO: 2024-11-10 21:45:34,781: llmtf.base.hfmodel: Updated generation_config.stop_strings: ['<|im_end|>']\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 1661.40it/s]\n",
      "INFO: 2024-11-10 21:45:36,882: llmtf.base.kngrg/ru-QAmeleon: Loading Dataset: 2.10s\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [02:39<00:00,  3.18s/it]\n",
      "INFO: 2024-11-10 21:48:16,042: llmtf.base.kngrg/ru-QAmeleon: Processing Dataset: 159.16s\n",
      "INFO: 2024-11-10 21:48:16,043: llmtf.base.kngrg/ru-QAmeleon: Results for kngrg/ru-QAmeleon:\n",
      "INFO: 2024-11-10 21:48:16,044: llmtf.base.kngrg/ru-QAmeleon: {'f1': 0.03360182205904116, 'em': 0.0}\n"
     ]
    }
   ],
   "source": [
    "from llmtf.evaluator import Evaluator\n",
    "evaluator = Evaluator()\n",
    "\n",
    "evaluator.evaluate_dataset(\n",
    "    task=task,\n",
    "    model=orig_model,\n",
    "    output_dir='./qa_qwen_orig',\n",
    "    max_len=4000,\n",
    "    few_shot_count=0,\n",
    "    generation_config=None, \n",
    "    batch_size=4,\n",
    "    max_sample_per_dataset=200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"task_name\": \"kngrg/ru-QAmeleon\",\n",
      "    \"results\": {\n",
      "        \"f1\": 0.03360182205904116,\n",
      "        \"em\": 0.0\n",
      "    },\n",
      "    \"leaderboard_result\": 0.01680091102952058\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!cat ./qa_qwen_orig/kngrg_ru-QAmeleon_total.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuClass": "premium",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
