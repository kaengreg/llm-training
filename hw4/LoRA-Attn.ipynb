{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zGiKaSVrkr90",
    "outputId": "78c5d02a-2b85-43ce-e9d4-7cbe32b96530"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Nov  9 17:19:15 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.4     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          Off | 00000000:CA:00.0 Off |                    0 |\n",
      "| N/A   47C    P0              97W / 400W |  26968MiB / 81920MiB |     88%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BJ3hpGEKHEpG",
    "outputId": "f3d6a630-530f-4208-fa4c-ec7411730dd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.1.0)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.4)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.4.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.23.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2024.2.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.6.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.5.40)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: transformers==4.45.2 in /usr/local/lib/python3.10/dist-packages (4.45.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.2) (3.14.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.2) (0.23.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.2) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.2) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.2) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.2) (2023.12.25)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.2) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.2) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.2) (0.20.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.2) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.2) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.2) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.2) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.2) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.2) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.2) (2024.2.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.43.1)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.4.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.23.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2.6.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->bitsandbytes) (12.5.40)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->bitsandbytes) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.18.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: rouge-score in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score) (2.1.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score) (3.8.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.23.5)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.16.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (4.66.5)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: pymorphy2 in /usr/local/lib/python3.10/dist-packages (0.9.1)\n",
      "Requirement already satisfied: dawg-python>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from pymorphy2) (0.7.2)\n",
      "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /usr/local/lib/python3.10/dist-packages (from pymorphy2) (2.4.417127.4579844)\n",
      "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.10/dist-packages (from pymorphy2) (0.6.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.13.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.4)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.4.0)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.45.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.66.5)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (1.1.0)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.23.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (3.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2024.2.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.6.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->peft) (12.5.40)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.20.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install accelerate\n",
    "!pip install transformers==4.45.2\n",
    "!pip install bitsandbytes\n",
    "!pip install datasets\n",
    "!pip install rouge-score\n",
    "!pip install pymorphy2\n",
    "!pip install peft\n",
    "#!pip install flash_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'llmtf_open' already exists and is not an empty directory.\n",
      "/workdir/diploma-llm/hw3/llmtf_open\n",
      "--2024-11-09 17:19:35--  https://raw.githubusercontent.com/dialogue-evaluation/RuOpinionNE-2024/master/train.jsonl\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1291979 (1.2M) [text/plain]\n",
      "Saving to: ‘train.jsonl.15’\n",
      "\n",
      "train.jsonl.15      100%[===================>]   1.23M  --.-KB/s    in 0.1s    \n",
      "\n",
      "2024-11-09 17:19:36 (10.9 MB/s) - ‘train.jsonl.15’ saved [1291979/1291979]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/RefalMachine/llmtf_open\n",
    "%cd llmtf_open\n",
    "!wget https://raw.githubusercontent.com/dialogue-evaluation/RuOpinionNE-2024/master/train.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "WU1F0zQHbNMk"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import List, Dict\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "class ChatDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        original_records: List[Dict],\n",
    "        tokenizer: AutoTokenizer,\n",
    "        max_tokens_count: int,\n",
    "        sample_rate: float = 1.0,\n",
    "        only_target_loss: bool = True,\n",
    "        add_global_bos: bool = True,\n",
    "        add_global_eos: bool = True,\n",
    "        labels_pad_token_id: int = -100,\n",
    "    ):\n",
    "        self.original_records = original_records\n",
    "        self.sample_rate = sample_rate\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_tokens_count = max_tokens_count\n",
    "        self.only_target_loss = only_target_loss\n",
    "        self.labels_pad_token_id = labels_pad_token_id\n",
    "        self.add_global_bos = add_global_bos\n",
    "        self.add_global_eos = add_global_eos\n",
    "        self.is_printed = False\n",
    "\n",
    "        self.records = []\n",
    "        for record in tqdm(original_records):\n",
    "            if random.random() > self.sample_rate:\n",
    "                continue\n",
    "            tensors = self.convert_record(record)\n",
    "            if tensors is None:\n",
    "                continue\n",
    "            self.records.append(tensors)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.records)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.records[index]\n",
    "\n",
    "    def fill_template(self, message: str, inputs: Dict[str, str]) -> str:\n",
    "        \"\"\"Заполняет шаблон значениями из inputs.\"\"\"\n",
    "        return message.format(**inputs)\n",
    "\n",
    "    def get_tokens(self, messages):\n",
    "        tokens = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_special_tokens=False,\n",
    "            tokenize=True,\n",
    "            add_generation_prompt=False,\n",
    "        )\n",
    "        if tokens[0] == self.tokenizer.bos_token_id:\n",
    "            tokens = tokens[1:]\n",
    "        return tokens\n",
    "\n",
    "    def convert_record(self, record):\n",
    "        input_ids, labels = [], []\n",
    "\n",
    "        filled_messages = [\n",
    "            {\"content\": self.fill_template(msg[\"content\"], record[\"inputs\"]), \"role\": msg[\"role\"]}\n",
    "            for msg in record[\"messages\"]\n",
    "        ]\n",
    "\n",
    "        for i, message in enumerate(filled_messages):\n",
    "            if message['role'] == 'bot':\n",
    "                message['role'] = 'assistant'\n",
    "                filled_messages[i]['role'] = 'assistant'\n",
    "\n",
    "            message_input_ids = self.get_tokens([message])\n",
    "            message_labels = message_input_ids\n",
    "\n",
    "            if len(input_ids) + len(message_input_ids) > self.max_tokens_count - 2:\n",
    "                break\n",
    "\n",
    "            labels_mask = [self.labels_pad_token_id for _ in range(len(message_input_ids))]\n",
    "            if message[\"role\"] not in (\"assistant\", \"bot\", \"gpt\") and self.only_target_loss:\n",
    "                message_labels = labels_mask\n",
    "\n",
    "            input_ids.extend(message_input_ids)\n",
    "            labels.extend(message_labels)\n",
    "\n",
    "        if not input_ids:\n",
    "            return None\n",
    "\n",
    "        original_input_ids = self.get_tokens(filled_messages)\n",
    "        if input_ids != original_input_ids[: len(input_ids)]:\n",
    "            print(\"Mismatch found:\")\n",
    "            print(\"Generated input_ids:\", input_ids)\n",
    "            print(\"Original input_ids:\", original_input_ids[: len(input_ids)])\n",
    "        \n",
    "        assert input_ids == original_input_ids[: len(input_ids)]\n",
    "\n",
    "        if self.add_global_bos and input_ids[0] != self.tokenizer.bos_token_id:\n",
    "            input_ids.insert(0, self.tokenizer.bos_token_id)\n",
    "            labels.insert(0, self.labels_pad_token_id)\n",
    "\n",
    "        if input_ids[-2] == self.tokenizer.eos_token_id:\n",
    "            input_ids = input_ids[:-1]\n",
    "            labels = labels[:-1]\n",
    "\n",
    "        if self.add_global_eos and input_ids[-1] != self.tokenizer.eos_token_id:\n",
    "            input_ids.append(self.tokenizer.eos_token_id)\n",
    "            labels.append(self.tokenizer.eos_token_id)\n",
    "\n",
    "        if not self.is_printed:\n",
    "            print(input_ids)\n",
    "            print(labels)\n",
    "            print(\n",
    "                \"Full prompt:\" +\n",
    "                self.tokenizer.decode(input_ids, skip_special_tokens=False)\n",
    "            )\n",
    "            assert '\\n' in self.tokenizer.decode(input_ids, skip_special_tokens=False)\n",
    "            self.is_printed = True\n",
    "\n",
    "        input_ids = torch.LongTensor(input_ids)\n",
    "        labels = torch.LongTensor(labels)\n",
    "        attention_mask = input_ids.new_ones(input_ids.size())\n",
    "        assert (\n",
    "            input_ids.size(0)\n",
    "            == labels.size(0)\n",
    "            == attention_mask.size(0)\n",
    "            <= self.max_tokens_count\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"labels\": labels,\n",
    "            \"attention_mask\": attention_mask,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "spC3RQKabQcB",
    "outputId": "de636200-30b0-497c-be50-72c7c56cfdbb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['messages', 'inputs', 'outputs'],\n",
       "        num_rows: 3689\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['messages', 'inputs', 'outputs'],\n",
       "        num_rows: 923\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('kngrg/ru-QAmeleon')\n",
    "dataset = dataset['test']\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "9tew9-G5ak4b"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorForTokenClassification,\n",
    "    AutoConfig,\n",
    ")\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    logging,\n",
    "    TrainerCallback,\n",
    "    TrainerState,\n",
    "    TrainerControl,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n",
    "from peft import get_peft_model, LoraConfig\n",
    "import re\n",
    "from peft import prepare_model_for_kbit_training\n",
    "import codecs\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import json\n",
    "import copy\n",
    "from collections import OrderedDict, defaultdict\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from datasets import load_dataset, Dataset\n",
    "from typing import Dict, List, Tuple\n",
    "from llmtf.metrics import mean, metric_max_over_ground_truths, f1_macro_score\n",
    "import transformers.data.metrics.squad_metrics as squad_metrics\n",
    "import re\n",
    "from llmtf.base import Task, SimpleFewShotHFTask, LLM\n",
    "from difflib import SequenceMatcher\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "class QATask(SimpleFewShotHFTask):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.method = 'generate'\n",
    "        self.dataset_name = 'QAmeleon'\n",
    "        self._max_new_tokens = 64\n",
    "\n",
    "    @classmethod\n",
    "    def name(cls):\n",
    "        return 'kngrg/ru-QAmeleon'\n",
    "\n",
    "    def dataset_args(self) -> Dict:\n",
    "        return {'path': 'kngrg/ru-QAmeleon'}\n",
    "\n",
    "    def aggregation(self) -> Dict:\n",
    "        return {\n",
    "          \"f1\": mean,\n",
    "          \"em\": mean\n",
    "        }\n",
    "\n",
    "    def evaluate(self, sample, y_pred) -> Dict:\n",
    "        y_true = sample['outputs']['segment']\n",
    "        f1 = metric_max_over_ground_truths(squad_metrics.compute_f1, y_pred, y_true)\n",
    "        em = metric_max_over_ground_truths(squad_metrics.compute_exact, y_pred, y_true)\n",
    "\n",
    "        return {\n",
    "          \"f1\": f1,\n",
    "          \"em\": em\n",
    "        }\n",
    "\n",
    "    def test_split_name(self) -> str:\n",
    "        return 'test'\n",
    "\n",
    "    def prompt_split_name(self) -> str:\n",
    "        return 'prompt'\n",
    "\n",
    "    def create_messages(self, sample, with_answer=None) -> List[Dict]:\n",
    "        messages = sample['messages']\n",
    "        inputs = sample['inputs']\n",
    "        for m in messages:\n",
    "            m['content'] = m['content'].format(**inputs)\n",
    "        return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = QATask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 177,
     "referenced_widgets": [
      "95ae65903e3f4ce1959576b04bf6c4ff",
      "bd97ac6e56af45e3bf4b995c345f533c",
      "52ff31b1c5a9442991671d95d1e23fd4",
      "4f201002103948c18e94f54fb8567848",
      "a701759536274f55bbf8320578b2b3e0",
      "8e3ede7f99e34b008668b088f2565dfe",
      "6c07b9b025d447788341b416fec53147",
      "9c7e848c539b427f96706079bd1e64a2",
      "cde8795db54c47519c1403f239a58a13",
      "f78f74f1c69d42f8a14882d8aeb160ee",
      "dde7bb1edcf7401d9749fdb1e52e864c",
      "9187851156794fa8b41c36796b947f71",
      "6d74fc562e70461aa011fbee5bd7665e",
      "3ea94754c8b34bb3aad14fb4c9f0e594",
      "033b47e9310a4a229fde29cb2064dc6c",
      "b1028e64e9774542a29ef139202d1288",
      "245ff375638e41c8ac1e47eebdeb2b01",
      "07505c425cd5419ab4a41c67ada392ae",
      "938cd3787f8548ec8b8a36d3774f9eec",
      "3e4faae35d4b45d1b30a32e6d885066e",
      "2a584e7dc2b34cf8801fc5a5a1d7740a",
      "171d46a6115f47cba78446ae6371957d",
      "238e6f2d486d4408b0a7adbe6f1889f6",
      "0b3b4d659ddc4e91893e981e6bbf8ed0",
      "8546ebfc80954e5cb04fb8ea362be249",
      "36bcf267ff9942649671cce81e742aba",
      "5684ac71a1c34ffc8ea47fc9710b2f1c",
      "49d0c021442e43a2bc44800d49a7b2d6",
      "a3ade70ced7047118ccc7539dc27ae14",
      "c4d1ed5fb4684e3abe7a4526b283332f",
      "d948f33771c14bc4b2cacfb52d3ed9b6",
      "5f957cdd9aeb465d87609f3fb1780bd4",
      "51533dec6af44373bcf5a1c8d86e04d8",
      "866573d83f43442f987a7bb85a050a6c",
      "70f208a6e2264253bccd851db498b99f",
      "29aa7376c1fe4c7189369dfabdb5ae1a",
      "6ae57b9d8a814d7c971e3b445957f769",
      "18fa7c870f884d8cb72f09854e994ccd",
      "df2e3ec3bc264250b2619e154ca72e22",
      "9be3d99d069d4413a76242b0a22f9ea4",
      "5edae9ebae214b119966fdd638653c10",
      "3d3345c8c7c443de904983c69249b595",
      "511b81915b0644bc8f440e71a39f5164",
      "8e1b8f22ea7d4f06bcf7cfe0cf21a122",
      "04618654b00d486fb0864c2392d1172c",
      "d080bdb1cb5c44e791db6865632c9159",
      "0e89d83db14b4c2fb0a42dc0863953fd",
      "8fbc6af2924549a6966534b49c87d17a",
      "8d885ce1464b4405b87cf31a0c030c65",
      "e441c3c1bd3f4b79afc01555419afcb2",
      "bb3158f06a1e4536b157f704047b77da",
      "45f60f10ef744353a2c0f6ebfd3a3689",
      "d9b1df824f744d98ac9fbd87e7fb14ef",
      "47b0ad5e8a404e9b8cc468b278e3872f",
      "9048bd8bb75b426da09aa0bb96246053"
     ]
    },
    "id": "sQJOjWMiXnf9",
    "outputId": "23a7582e-f9bf-4e2f-e7e4-287198573e6c"
   },
   "outputs": [],
   "source": [
    "model_name = 'RefalMachine/ruadapt_qwen2.5_3B_ext_u48_instruct_v4'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rTCT7ZxJdbgU",
    "outputId": "9a29549b-6186-4432-f5f4-03f30f0d9803"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|████████▎                                                                                                                                                                                                            | 144/3689 [00:00<00:04, 738.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[147076, 882, 198, 54745, 38438, 9542, 102073, 105116, 71995, 7740, 93747, 102833, 13373, 100594, 25, 101036, 5927, 100570, 30889, 102847, 100431, 108537, 220, 20, 38, 949, 11712, 35095, 15298, 6735, 4708, 1232, 23784, 220, 17, 15, 16, 24, 100281, 13875, 17, 30889, 126119, 112058, 44065, 102849, 5927, 107383, 39988, 104109, 220, 20, 38, 5927, 132230, 100457, 106239, 100817, 13373, 120356, 104306, 13, 127333, 106645, 127131, 5927, 101237, 112058, 44786, 102496, 37405, 108204, 84198, 7820, 136822, 220, 19, 38, 7740, 220, 20, 38, 11, 20440, 121024, 100342, 107421, 1506, 98058, 111349, 100871, 102819, 44858, 102141, 13373, 112058, 39988, 123816, 13, 100792, 92878, 110777, 105989, 103399, 118043, 105615, 101900, 220, 16, 61565, 101981, 2971, 5524, 104047, 100346, 57297, 220, 20, 11562, 2297, 627, 147077, 198, 147076, 78191, 198, 60627, 48074, 25, 147077]\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 147076, 78191, 198, 60627, 48074, 25, 147077]\n",
      "Full prompt:<|im_start|>user\n",
      "Прочитайте данный текст и ответьте на вопрос: Когда в России запустились услуги 5G ? '''Текст''': В 2019 году Tele2 запустила облачные игры в пилотной сети 5G в флагманском салоне на Тверской улице. Оператор показал разницу в качестве облачных игр при подключении к сетям 4G и 5G, для демонстрации была выбрана операционно требовательная игра на облачной платформе. Во время испытаний технологии компания достигла скорости выше 1 Гбит/c с задержкой до 5 мс.\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Ответ:<|im_end|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3689/3689 [00:04<00:00, 769.01it/s]\n",
      " 17%|███████████████████████████████████▍                                                                                                                                                                                  | 153/923 [00:00<00:01, 765.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[147076, 882, 198, 54745, 38438, 9542, 102073, 105116, 71995, 7740, 93747, 102833, 13373, 100594, 25, 117310, 105429, 104230, 16331, 101967, 101342, 105358, 1482, 949, 11712, 35095, 15298, 6735, 4708, 1232, 105325, 104112, 113059, 101342, 105358, 1482, 132493, 13373, 108387, 5524, 128706, 104230, 16331, 101967, 101372, 110313, 11, 13373, 113123, 4194, 2345, 5524, 116655, 50807, 107987, 12, 108323, 38972, 12, 100046, 12, 129098, 28089, 17721, 7740, 104001, 12, 101037, 18437, 19479, 25657, 11, 13373, 108956, 4194, 2345, 5524, 116655, 12507, 104001, 12, 101037, 3865, 7486, 12, 20807, 67124, 12507, 44155, 73281, 11, 13373, 111379, 4194, 2345, 5524, 128706, 114997, 106480, 42047, 51412, 13, 112266, 104230, 16331, 101967, 102655, 220, 18, 15, 17, 101405, 30556, 627, 147077, 198, 147076, 78191, 198, 60627, 48074, 25, 147077]\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 147076, 78191, 198, 60627, 48074, 25, 147077]\n",
      "Full prompt:<|im_start|>user\n",
      "Прочитайте данный текст и ответьте на вопрос: Какая площадь муниципалитета Самбрано ? '''Текст''': Муниципалитет Самбрано граничит на юге с территорией муниципалитета Кордова, на западе — с муниципалитетами Эль-Кармен-де-Боливар и Сан-Хасинто, на севере — с муниципалитетом Сан-Хуан-Непомусено, на востоке — с территорией департамента Магдалена. Площадь муниципалитета составляет 302 км².\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Ответ:<|im_end|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 923/923 [00:01<00:00, 758.76it/s]\n"
     ]
    }
   ],
   "source": [
    "only_target_loss = True\n",
    "max_tokens_count = 512\n",
    "datasets = []\n",
    "for records in (dataset['train'], dataset['test']):\n",
    "    datasets.append(\n",
    "        ChatDataset(\n",
    "            records,\n",
    "            tokenizer,\n",
    "            max_tokens_count=max_tokens_count,\n",
    "            sample_rate=1.0,\n",
    "            only_target_loss=only_target_loss,\n",
    "            add_global_eos=False,\n",
    "            add_global_bos=False\n",
    "        )\n",
    "    )\n",
    "train_dataset, val_dataset = datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 710,
     "referenced_widgets": [
      "5d35aa519ce8413c8923084acdcfa499",
      "a0b54e4a8750456385e80446d7f54601",
      "015077447f0a46c2a37d2f875ad5b53e",
      "bb1fd8ed1914441d931833fc62169067",
      "9cb81f6968324d5ab4f78a64cedfb9e5",
      "be95cfb33136480f93990d7566a27911",
      "25fc22c63f984905a397e4ac8b8662b1",
      "9cac04e3d3f24f97abf114d98d50394c",
      "3d6d74fe2b1f4b61a14250194519601d",
      "9c51a5cdd0b44db48be7d1de0c76c2f9",
      "10c10dc39b3d4adbba73346065d8b73b",
      "8f232fb0ede84de9a365b6b726ba8cbb",
      "1440876e8d6c43aaa29f407c68eb5cd1",
      "3464e70c2f854b6392ed5a1bfb02daca",
      "1f8c68438fa94f8c8496be48a2d55f64",
      "ccf91b319ee24a94811997bb9f024d02",
      "3cc217df57ab40979bf4f119929b38f1",
      "3abb8498062540879e3ef6c20e9774f7",
      "5bb378b3714b48f59656ad4e7621733a",
      "a362656a5e3f45cca119b561e6b4a350",
      "c100d160848e4512aac018a45eec6903",
      "635d5e788d864557984eea6eacfa0b0d",
      "207dd3c14d8e4d8fb97b7a3b42ad9227",
      "d841378207f44ccd820388f60685717d",
      "29243643d89f4e0ca319fec7b8118dfb",
      "f203243bd12b48dda479d9f7363d8807",
      "a1a6a50ee0a147a6958b01091793b372",
      "5b336dc9eee14fb7a7afe059962b73c8",
      "efee8074b0ea4ef6b031d4658589da88",
      "4e9bb8cf54754924b8023b6e3201d9a9",
      "35f71306b4d8476795c78f594b05587b",
      "ecd9d5e549c14b1a97f91bf4ea9093cb",
      "ea7e4a62f8d04b5286545bf249c87236",
      "e18a89cccb9447c1969ff5caba7758fe",
      "a3d3a4fca96641558b35ef6703652ad6",
      "5b4b8e10185b4547bf8c48534aa7d55e",
      "e3530f9303614232968f1a7778733404",
      "9c6838e7f54a421b9452294bad074457",
      "ba5b0493fcab446aac1205538f84a6a4",
      "5b6e97fd6a5e44e492de8cba0e02291a",
      "3dccaf2444c64bb39f07a99d37121387",
      "ccdba3991b3441428bcbe760bbce08b4",
      "4c330fd683ca410f86d66cb33218f615",
      "1badc901c3994c63a579c5afb165c3a8",
      "7fe4a42fa02c4c2bb8e97d4f92ea6457",
      "161cb58c79a744848c149fbd5f88e6cb",
      "6c58a865fbf6432d97e9fa38c9875c26",
      "ad60c25ad4654dde835fbcce4c1b2f93",
      "263138fb51fe465ebf441a8986afe551",
      "3b76284c89de454a88b286bc5f7ce543",
      "f6c05e9daed1457a81b9d44b51acf283",
      "9bc8453ba9524369badf279778231371",
      "32486bf6a27548489659cd26ea2e38b5",
      "efe203f606ea45ad8c8b543b9be3b33f",
      "b05f9df8b3a94f548c1fe7604890ac5b",
      "10338be0394440c1a448571d2e1af417",
      "94ff4351ed5344a4a9cfc76401a1eabd",
      "938eabc2e6ab41639bd7ee7e078dd15e",
      "737947e6a69847a7bfb8fdfb9d40c482",
      "528f9cd5eef2417eaf9e55fac60a623d",
      "31380661c10f439ab842d526ed7c9eb4",
      "90b8f256b1eb4c0086dd83a1d79c27c1",
      "dbadabaf90324f9e9784623c31140aef",
      "7cccd19e14ac46d4878a35d747dd0d98",
      "3767e6894d2b47cebf2c240e11f9aa78",
      "e7599b1f248b4e4398218428dd0661a0",
      "2245907688e245d58a0ec34191ceec95",
      "88903effe0fd4d12b69c4aeb193db6f5",
      "c03850fd8a9a438590f1be2643d08dbb",
      "b61c25883f9e4a73b017bbff3296e369",
      "4a39f82ff9cf4cbd992f7e22d173f7ea",
      "731dadaa89f442239975d39d44d6d937",
      "7ed59602330b47ca97ea3b1d194131ba",
      "a8e090da89194ca7986cc4e96184dd7d",
      "fd4fde04ae064c2d8c582822e4ea6b9a",
      "88f7c4a4c38c4b66b2f91340abb8fc05",
      "07a84434bf6c4f22bb76c2a513a8fb27"
     ]
    },
    "id": "WO-tdmbI5Vwg",
    "outputId": "83ac8fcf-4ee8-41fe-8a6c-66f7717c7bcc"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5373c28e86da47c691a1aad6cba42ce8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(147097, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-35): 36 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2SdpaAttention(\n",
       "          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=2048, out_features=256, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=2048, out_features=256, bias=True)\n",
       "          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): Qwen2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear4bit(in_features=2048, out_features=11008, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=2048, out_features=11008, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=11008, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=147097, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=f\"cuda:0\",\n",
    "    torch_dtype=torch.float16,\n",
    "    attn_implementation=\"sdpa\",\n",
    ")\n",
    "prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ytjHd1qxfaHq",
    "outputId": "bef5bfc5-daba-45fc-e318-f1c49221fe21"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[147076,    882,    198,  20807,  19619, 129192,  48489, 101765,    445,\n",
       "          11237,     13, 147077,    198, 147076,  78191,    198]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [{'role': 'user', 'content': 'Напиши что такое LLM.'}]\n",
    "tokenizer.apply_chat_template(messages, return_tensors='pt', add_special_tokens=True, add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OsRI8Tj4fOwA",
    "outputId": "a8f8cace-74d5-4f82-8774-5180df1eaf3e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerationConfig {\n",
       "  \"bos_token_id\": 147075,\n",
       "  \"do_sample\": true,\n",
       "  \"eos_token_id\": 147077,\n",
       "  \"max_new_tokens\": 64,\n",
       "  \"pad_token_id\": 147075,\n",
       "  \"temperature\": 0.2,\n",
       "  \"top_k\": 40,\n",
       "  \"top_p\": 0.9\n",
       "}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GenerationConfig\n",
    "\n",
    "def generate(messages, model, tokenizer, generation_config):\n",
    "    input_ids = tokenizer.apply_chat_template(messages, return_tensors='pt', add_special_tokens=True, add_generation_prompt=True)\n",
    "    input_ids = input_ids.to(model.device)\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            input_ids,\n",
    "            generation_config=generation_config\n",
    "        )\n",
    "    outputs = []\n",
    "    for sample_output_ids, sample_input_ids in zip(output_ids, input_ids):\n",
    "        sample_output_ids = sample_output_ids[len(sample_input_ids):]\n",
    "        sample_output = tokenizer.decode(sample_output_ids, skip_special_tokens=True)\n",
    "        outputs.append(sample_output)\n",
    "\n",
    "    if len(outputs) == 1:\n",
    "        outputs = outputs[0]\n",
    "    return outputs\n",
    "\n",
    "\n",
    "\n",
    "generation_config = GenerationConfig.from_dict(\n",
    "    {\n",
    "        'top_k': 40,\n",
    "        'top_p': 0.9,\n",
    "        'temperature': 0.2,\n",
    "        'repetition_penalty': 1.0,\n",
    "        'max_new_tokens': 64,\n",
    "        'do_sample': True,\n",
    "        'pad_token_id': tokenizer.pad_token_id,\n",
    "        'bos_token_id': tokenizer.bos_token_id,\n",
    "        'eos_token_id': tokenizer.eos_token_id\n",
    "    }\n",
    ")\n",
    "generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "r6QZaeDHfwmo",
    "outputId": "ca2c5fd8-feed-4974-c41f-81b2cc198555"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LLM (Law Legal Model) — это искусственный интеллект, разработанный для выполнения юридических задач, таких как анализ правовых документов, определение правовых положений, предоставление рекомендаций по юридическим вопросам и другие. Эти системы обучаются на большом объеме данных, связанных с юриспруденцией, включая законы, судебные решения, юридические статьи и другие источники'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(messages, model, tokenizer, generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Rid85doezUI",
    "outputId": "2693344f-988e-40e8-8023-8a4d5bf0bcbb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type=None, inference_mode=False, r=16, target_modules={'q_proj', 'k_proj', 'v_proj', 'o_proj'}, lora_alpha=16, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_config = {\n",
    "    \"r\": 16,\n",
    "    \"lora_alpha\": 16,\n",
    "    \"lora_dropout\": 0.0,\n",
    "    \"bias\": \"none\",\n",
    "    \"target_modules\": [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n",
    "}\n",
    "lora_config = LoraConfig(**lora_config)\n",
    "lora_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "NdRq7JpKgFrv"
   },
   "outputs": [],
   "source": [
    "lora_config.modules_to_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "jsK7nWcdeiDn"
   },
   "outputs": [],
   "source": [
    "model = get_peft_model(model, lora_config)\n",
    "if model.config.tie_word_embeddings and lora_config.modules_to_save is not None and 'lm_head' in lora_config.modules_to_save:\n",
    "    print('Tie embeddings')\n",
    "    assert 'embed_tokens' not in lora_config.modules_to_save\n",
    "    model.base_model.model.model.embed_tokens.weight = model.base_model.model.lm_head.modules_to_save[\"default\"].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qs4VhdlYgWNh",
    "outputId": "c69e8525-e971-438b-da23-ee14c68a0cb3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workdir/miniconds3/envs/llm-course/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "training_args = {\n",
    "    \"evaluation_strategy\": \"steps\",\n",
    "    \"per_device_train_batch_size\": 1,\n",
    "    \"per_device_eval_batch_size\": 1,\n",
    "    \"gradient_accumulation_steps\": 8,\n",
    "    \"eval_steps\": 16,\n",
    "    \"save_steps\": 128,\n",
    "    \"logging_steps\": 1,\n",
    "    \"learning_rate\": 0.00005,\n",
    "    \"num_train_epochs\": 1,\n",
    "    \"lr_scheduler_type\": \"cosine\",\n",
    "    \"warmup_steps\": 16,\n",
    "    \"bf16\": False,\n",
    "    \"fp16\": True,\n",
    "    \"optim\": \"paged_adamw_8bit\",\n",
    "    \"save_total_limit\": 1,\n",
    "    \"seed\": 1337,\n",
    "    \"max_grad_norm\": 1.0,\n",
    "    \"weight_decay\": 0.05\n",
    "}\n",
    "training_args = TrainingArguments(output_dir='./instruct', **training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LLoHlUYMgH3p",
    "outputId": "9bbc6d24-425a-4761-889b-ccea3f7374de"
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=8)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "if len(trainer.label_names) == 0:\n",
    "    trainer.label_names.append('labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "id": "smSpQW4BgwW2",
    "outputId": "8b3069f4-290e-47f4-ae61-a3dc6839d0bb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/workdir/miniconds3/envs/llm-course/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='461' max='461' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [461/461 1:25:03, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>3.192400</td>\n",
       "      <td>3.111310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.483100</td>\n",
       "      <td>0.353903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.009200</td>\n",
       "      <td>0.008298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>0.001724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.001024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.000636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.000553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.000491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.000443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.000403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.000371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>208</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>272</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>304</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>336</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>352</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>368</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>384</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>416</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>432</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>448</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000236</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workdir/miniconds3/envs/llm-course/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/workdir/miniconds3/envs/llm-course/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/workdir/miniconds3/envs/llm-course/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=461, training_loss=0.18626474837451584, metrics={'train_runtime': 5107.6801, 'train_samples_per_second': 0.722, 'train_steps_per_second': 0.09, 'total_flos': 7832566081781760.0, 'train_loss': 0.18626474837451584, 'epoch': 0.9997289238275956})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "id": "S5dxw71jhCSm",
    "outputId": "e4489a55-bc51-4d68-8111-63b9410eb027"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workdir/miniconds3/envs/llm-course/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/workdir/miniconds3/envs/llm-course/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'LLM (Law Master of Laws) — это профессиональный диплом, который присуждается студентам после успешного окончания четырех- или пяти-летний программы обучения в области права. ЛLM-специализация позволяет студентам углубить свои знания и навыки в определенной области права, например, в международном праве, корпоративном праве, интеллекту'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(messages, model, tokenizer, generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Nn814PE5h70t"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('models/ruQwen-lora/tokenizer_config.json',\n",
       " 'models/ruQwen-lora/special_tokens_map.json',\n",
       " 'models/ruQwen-lora/vocab.json',\n",
       " 'models/ruQwen-lora/merges.txt',\n",
       " 'models/ruQwen-lora/added_tokens.json',\n",
       " 'models/ruQwen-lora/tokenizer.json')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "save_directory = \"models/ruQwen-lora\"\n",
    "\n",
    "model.save_pretrained(save_directory)\n",
    "tokenizer.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = QATask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62a260a5087d499f8128911181c6a180",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: 2024-11-09 18:45:21,319: llmtf.base.hfmodel: Set eos_token_id in generation_config to [147077]\n",
      "WARNING: 2024-11-09 18:45:21,320: llmtf.base.hfmodel: Global prefix is equal to empty string!\n",
      "INFO: 2024-11-09 18:45:21,321: llmtf.base.hfmodel: Model id: models/ruQwen-lora\n",
      "INFO: 2024-11-09 18:45:21,321: llmtf.base.hfmodel: Leading space: False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GenerationConfig {\n",
       "  \"bos_token_id\": 147075,\n",
       "  \"eos_token_id\": [\n",
       "    147077\n",
       "  ],\n",
       "  \"max_length\": 32768,\n",
       "  \"max_new_tokens\": 200,\n",
       "  \"pad_token_id\": 147075,\n",
       "  \"stop_strings\": [\n",
       "    \"<|im_end|>\"\n",
       "  ],\n",
       "  \"temperature\": 0.0,\n",
       "  \"top_k\": 40,\n",
       "  \"top_p\": 0.9\n",
       "}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llmtf.model import HFModel\n",
    "\n",
    "model_name_or_path = 'models/ruQwen-lora'\n",
    "model = HFModel(conversation_template_path='conversation_configs/qwen2.json', device_map='cuda:0', attn_implementation=\"sdpa\")\n",
    "model.from_pretrained(model_name_or_path)\n",
    "\n",
    "model.generation_config.max_new_tokens = 200\n",
    "model.generation_config.repetition_penalty = 1.0\n",
    "model.generation_config.do_sample = False\n",
    "model.generation_config.temperature = 0.0\n",
    "model.generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: 2024-11-09 18:45:21,331: llmtf.base.hfmodel: Updated generation_config.eos_token_id: [147077]\n",
      "INFO: 2024-11-09 18:45:21,332: llmtf.base.hfmodel: Updated generation_config.stop_strings: ['<|im_end|>']\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 1648.19it/s]\n",
      "INFO: 2024-11-09 18:45:24,007: llmtf.base.kngrg/ru-QAmeleon: Loading Dataset: 2.67s\n",
      "  0%|                                                                                                                                                                                                                                  | 0/50 [00:00<?, ?it/s]/workdir/miniconds3/envs/llm-course/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/workdir/miniconds3/envs/llm-course/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/workdir/miniconds3/envs/llm-course/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:612: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:03<00:00, 13.72it/s]\n",
      "INFO: 2024-11-09 18:45:27,652: llmtf.base.kngrg/ru-QAmeleon: Processing Dataset: 3.64s\n",
      "INFO: 2024-11-09 18:45:27,653: llmtf.base.kngrg/ru-QAmeleon: Results for kngrg/ru-QAmeleon:\n",
      "INFO: 2024-11-09 18:45:27,654: llmtf.base.kngrg/ru-QAmeleon: {'f1': 0.425, 'em': 0.425}\n"
     ]
    }
   ],
   "source": [
    "from llmtf.evaluator import Evaluator\n",
    "evaluator = Evaluator()\n",
    "\n",
    "evaluator.evaluate_dataset(\n",
    "    task=task,\n",
    "    model=model,\n",
    "    output_dir='./qa_qwenFT',\n",
    "    max_len=4000,\n",
    "    few_shot_count=0,\n",
    "    generation_config=None, \n",
    "    batch_size=4,\n",
    "    max_sample_per_dataset=200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"task_name\": \"kngrg/ru-QAmeleon\",\n",
      "    \"results\": {\n",
      "        \"f1\": 0.425,\n",
      "        \"em\": 0.425\n",
      "    },\n",
      "    \"leaderboard_result\": 0.425\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!cat ./qa_qwenFT/kngrg_ru-QAmeleon_total.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a65af1d422da4b7785a305319ef79402",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: 2024-11-09 18:45:31,349: llmtf.base.hfmodel: Set eos_token_id in generation_config to [147077]\n",
      "WARNING: 2024-11-09 18:45:31,350: llmtf.base.hfmodel: Global prefix is equal to empty string!\n",
      "INFO: 2024-11-09 18:45:31,350: llmtf.base.hfmodel: Model id: RefalMachine/ruadapt_qwen2.5_3B_ext_u48_instruct_v4\n",
      "INFO: 2024-11-09 18:45:31,350: llmtf.base.hfmodel: Leading space: False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GenerationConfig {\n",
       "  \"bos_token_id\": 147075,\n",
       "  \"eos_token_id\": [\n",
       "    147077\n",
       "  ],\n",
       "  \"max_length\": 32768,\n",
       "  \"max_new_tokens\": 200,\n",
       "  \"pad_token_id\": 147075,\n",
       "  \"stop_strings\": [\n",
       "    \"<|im_end|>\"\n",
       "  ],\n",
       "  \"temperature\": 0.0,\n",
       "  \"top_k\": 40,\n",
       "  \"top_p\": 0.9,\n",
       "  \"trust_remote_code\": false\n",
       "}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llmtf.model import HFModel\n",
    "\n",
    "model_name_or_path = 'RefalMachine/ruadapt_qwen2.5_3B_ext_u48_instruct_v4'\n",
    "orig_model = HFModel(conversation_template_path='conversation_configs/qwen2.json', device_map='cuda:0', attn_implementation=\"sdpa\")\n",
    "orig_model.from_pretrained(model_name_or_path)\n",
    "\n",
    "orig_model.generation_config.max_new_tokens = 200\n",
    "orig_model.generation_config.repetition_penalty = 1.0\n",
    "orig_model.generation_config.do_sample = False\n",
    "orig_model.generation_config.temperature = 0.0\n",
    "orig_model.generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: 2024-11-09 18:45:31,357: llmtf.base.hfmodel: Updated generation_config.eos_token_id: [147077]\n",
      "INFO: 2024-11-09 18:45:31,357: llmtf.base.hfmodel: Updated generation_config.stop_strings: ['<|im_end|>']\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 1658.74it/s]\n",
      "INFO: 2024-11-09 18:45:33,296: llmtf.base.kngrg/ru-QAmeleon: Loading Dataset: 1.94s\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [02:39<00:00,  3.19s/it]\n",
      "INFO: 2024-11-09 18:48:12,633: llmtf.base.kngrg/ru-QAmeleon: Processing Dataset: 159.34s\n",
      "INFO: 2024-11-09 18:48:12,634: llmtf.base.kngrg/ru-QAmeleon: Results for kngrg/ru-QAmeleon:\n",
      "INFO: 2024-11-09 18:48:12,635: llmtf.base.kngrg/ru-QAmeleon: {'f1': 0.03360182205904116, 'em': 0.0}\n"
     ]
    }
   ],
   "source": [
    "from llmtf.evaluator import Evaluator\n",
    "evaluator = Evaluator()\n",
    "\n",
    "evaluator.evaluate_dataset(\n",
    "    task=task,\n",
    "    model=orig_model,\n",
    "    output_dir='./qa_qwen_orig',\n",
    "    max_len=4000,\n",
    "    few_shot_count=0,\n",
    "    generation_config=None, \n",
    "    batch_size=4,\n",
    "    max_sample_per_dataset=200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"task_name\": \"kngrg/ru-QAmeleon\",\n",
      "    \"results\": {\n",
      "        \"f1\": 0.03360182205904116,\n",
      "        \"em\": 0.0\n",
      "    },\n",
      "    \"leaderboard_result\": 0.01680091102952058\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!cat ./qa_qwen_orig/kngrg_ru-QAmeleon_total.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuClass": "premium",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
